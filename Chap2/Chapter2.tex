\chapter{Background Study}
\label{chap:2}

\section{Introduction}

Online education was already growing before 2020, but the pandemic accelerated adoption dramatically. When we began researching this project, EDUCAUSE's 2021 survey showed that nearly three-quarters of higher education institutions worldwide had launched fully online programs—representing over 35 million students taking remote exams annually \cite{educause2021}. That's a massive population taking assessments outside traditional proctored classrooms.

The integrity problem became impossible to ignore. King et al. (2020) compared cheating rates across delivery formats and found something striking: online examination fraud runs 12-18\% higher than in-person assessments when proper proctoring mechanisms aren't in place \cite{king2020}. That gap isn't small—it represents millions of compromised exam results.

This reality shaped our project direction. We saw an opportunity to apply recent advances in artificial intelligence—particularly facial recognition, behavioral tracking, and object detection—to create real-time monitoring systems that could match or exceed human proctor capabilities. The goal wasn't just to detect cheating but to do it automatically at scale, reducing dependency on expensive human labor while actually improving security and responsiveness.

\section{Related Work}

\subsection{Computer Vision Technologies}

Computer vision for exam proctoring isn't new, but the technology has matured considerably in recent years. Early work by Atoum et al. (2017) demonstrated that automated proctoring using computer vision could achieve 78\% detection accuracy \cite{atoum2017}—decent, but not quite good enough for production deployment where false positives create serious consequences.

What changed? Mainly the efficiency of object detection models. We evaluated several options and ultimately selected YOLOv8n, released by Ultralytics in 2023 \cite{ultralytics2023}. The "n" stands for "nano"—it's the smallest variant in the YOLOv8 family. What impressed us was the performance-to-efficiency ratio. Unlike earlier YOLO versions that demanded GPU acceleration, YOLOv8n runs on CPU-only systems at 6-7 FPS while achieving 85-90\% detection accuracy. For educational institutions without machine learning infrastructure, this was crucial. They could deploy AI-powered proctoring on standard web servers.

For facial analysis, we turned to MediaPipe. Google Research developed it as a cross-platform framework for various machine learning tasks \cite{google2023}. The face detection component tracks 468 distinct facial landmarks—more than enough data to estimate gaze direction and head pose. Lugaresi et al. (2019) validated MediaPipe's accuracy at around 95\% in real-time scenarios \cite{lugaresi2019}, which aligned with our own testing. We use those 468 landmark coordinates to determine whether a student is looking at their screen, glancing at notes off to the side, or reading from a phone below the camera's primary field of view.

\subsection{Real-Time Communication}

Getting video from students' browsers to our AI processing servers required careful protocol selection. We needed something browser-native (no plugins), encrypted by default, and proven to scale.

WebRTC checked all three boxes. It's built directly into modern browsers for peer-to-peer communication \cite{webrtc2023}. Latency typically runs between 200-500ms—fast enough that video feels live rather than delayed. Security comes standard through DTLS-SRTP encryption, so video streams can't be intercepted in transit. Commercial platforms like ProctorU have validated WebRTC's scalability, routinely handling thousands of concurrent exam sessions \cite{proctoru2022}.

Where we diverged from typical implementations was in supporting dual WebRTC streams simultaneously. Most systems connect a single camera. We establish two separate WebRTC peer connections—one for the desktop webcam, another for the mobile phone camera. This required synchronization logic to ensure violation detection considers both feeds coherently rather than treating them as independent streams that might conflict.

For pushing alerts back to instructors, we chose Socket.IO. It handles bidirectional communication elegantly and delivers messages with 50-150ms latency \cite{rauch2016}. That matters because real-time means real-time—if a student pulls out their phone, we want the instructor notified within a fraction of a second, not several seconds later after they've already found the answer they were looking for.

\subsection{Authentication and Security}

Authentication posed an interesting architectural challenge. Traditional session-based authentication stores state on the server—who's logged in, what their permissions are, when their session expires. That approach works fine for monolithic applications but creates headaches when you're trying to scale horizontally across multiple server instances.

We adopted JSON Web Tokens instead \cite{jones2015}. JWTs are stateless—all the information needed to validate a user's identity and permissions is encoded directly in the token itself. When a student logs in, they receive a signed JWT. Every subsequent API request includes that token. Any server instance can validate it independently without checking a central session database. This makes horizontal scaling trivial—just spin up more servers behind a load balancer.

We implemented three-tier role-based access control on top of JWT: students can take exams and view their results, teachers can create exams and monitor sessions, administrators can manage users and system settings. Each JWT encodes the user's role, and API endpoints check that role before processing requests.

Privacy regulations shaped our security architecture from the beginning. The European Union's GDPR is probably the strictest \cite{gdpr2016}, mandating explicit consent before processing personal data, data minimization (don't collect more than you need), and the right to deletion (users can demand you erase their data). California's CCPA and the U.S. education-specific FERPA impose similar requirements.

We built compliance in rather than retrofitting it later. The platform requires explicit consent before activating cameras. We collect only the data necessary for proctoring—no extraneous information. Data retention is configurable, with automatic deletion after the specified period. All network traffic uses HTTPS/TLS encryption. Data at rest gets AES-256 encryption. Biometric data like facial landmarks receives special protection with additional access logging.

\subsection{Comparative Analysis of Existing Proctoring Platforms}

\subsubsection{Commercial Proctoring Services}

Before building our platform, we analyzed the existing commercial market to understand what worked, what didn't, and where opportunities existed for improvement. Five major players dominate online proctoring: ProctorU, Respondus Monitor, Examity, Proctorio, and a handful of smaller competitors.

Each takes a different approach. ProctorU uses live human proctors who watch students via webcam in real-time. Respondus records sessions for post-exam review rather than monitoring live. Examity offers a hybrid model combining AI flagging with human review. Proctorio relies entirely on automated AI detection. Table 2.1 summarizes the key differences:

\begin{table}[ht]
\centering
\caption{Comparative Analysis of Commercial Proctoring Platforms}
\label{tab:proctoring_comparison}
\begin{tabular}{|p{3cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{3cm}|}
\hline
\textbf{Platform} & \textbf{Proctoring Type} & \textbf{Cost/Exam} & \textbf{Real-Time Alerts} & \textbf{Key Limitations} \\
\hline
ProctorU & Live Human & \$17.50-\$42 & Yes & Privacy concerns, high cost \\
\hline
Respondus Monitor & Recorded Review & \$6-\$12 & No & Delayed detection, no intervention \\
\hline
Examity & Hybrid AI+Human & \$15-\$30 & Partial & High false-positives (18-25\%) \\
\hline
Proctorio & Automated AI & \$10-\$25 & Yes & Privacy issues, browser-only \\
\hline
\textbf{Our Platform} & \textbf{Automated AI} & \textbf{\$0 (Self-hosted)} & \textbf{Yes (<150ms)} & \textbf{Requires dual cameras} \\
\hline
\end{tabular}
\end{table}

\subsubsection{Comparative Advantages}

Looking at Table 2.1, several patterns emerge. First, live human proctoring (ProctorU) is expensive—\$17.50 to \$42 per exam adds up fast. Second, recorded review systems (Respondus) miss the whole point of real-time intervention. Third, hybrid systems (Examity) still carry high costs and suffer from false-positive rates between 18-25\%. Fourth, existing AI-only systems (Proctorio) face serious privacy criticisms and typically work only within specific browsers.

Our platform was designed to address these specific limitations. We use an open-source architecture that institutions can self-host, eliminating per-exam fees entirely. The dual-camera setup provides comprehensive monitoring without purchasing additional hardware—students use devices they already own. AI-powered detection runs in real-time with under 150ms alert latency, matching ProctorU's responsiveness without the human labor costs. Unlike competitors that focus solely on proctoring, we integrate complete exam management—question authoring, delivery, grading, and results—into a single platform. Privacy came first: explicit GDPR and CCPA compliance with transparent data handling. And the system runs entirely in browsers, working across platforms without plugins or specialized software.

\section{System Challenges and Solutions}

\subsection{Real-Time Data Processing}

One of our first reality checks came when we calculated bandwidth and processing requirements. A single 720p video stream running at 30 FPS generates roughly 500 MB of data per hour. Multiply that by hundreds or thousands of concurrent exam sessions and you're looking at terabytes of real-time data flow. Processing all those frames through AI models in real-time seemed daunting.

We tackled this through several optimizations. First, adaptive video quality: the system negotiates resolution (240p to 720p) based on each student's available network bandwidth. Students with slower connections get lower resolution, which is fine—we can still detect violations at 240p. Second, we don't process every frame. The AI models analyze 6-7 frames per second, not 30. For violation detection, 6 FPS provides plenty of temporal resolution. Third, we resize frames to 416x416 pixels before feeding them to YOLOv8n—the model runs faster on smaller inputs without significant accuracy loss. Fourth, processing happens in 150ms intervals, giving the CPU time to complete inference before the next batch arrives. Finally, we implemented multi-threading to distribute AI inference across all available CPU cores rather than bottlenecking on a single thread.

\subsection{AI Model Accuracy}

Off-the-shelf AI models work reasonably well, but they're trained on general-purpose datasets. YOLOv8n knows what a "cell phone" looks like in typical photos, but exam contexts are different—cameras positioned above the desk, unusual angles, partial occlusions, varied lighting. Similarly, facial recognition can degrade significantly in poor lighting or when students wear glasses that create reflections.

We addressed accuracy through several mechanisms. For object detection, we started with the pre-trained YOLOv8n model but fine-tuned it on examination-specific images—photos of phones held below desks, tablets at various angles, notes positioned near keyboards. This improved detection in realistic exam scenarios. For facial analysis, MediaPipe proved remarkably robust, but we still set a 70\% minimum confidence threshold—if face detection drops below that, the system alerts instructors that monitoring quality has degraded rather than silently failing.

Temporal consistency helped reduce false positives. Instead of triggering alerts on single-frame detections, we validate across a 20-frame history. If YOLOv8n detects a phone in one frame but not the surrounding nineteen frames, it's probably a false positive and we suppress the alert. We also implemented dynamic accuracy scoring that runs from 0-100\%, giving instructors transparency into how confident the AI is about each violation. Finally, violations are priority-classified: detecting multiple people is high-priority, detecting a questionable object shape is lower priority.

\subsection{System Scalability}

Scalability concerned us from the start. Every exam session consumes resources: video processing threads, database connections for storing violations, WebSocket connections for real-time alerts. If scaling required adding proportionally more servers for each additional student, the economics would break down quickly.

Our architecture was designed for horizontal scaling. The backend API is completely stateless thanks to JWT authentication—no session storage means any server can handle any request. Socket.IO supports server clustering out of the box, so multiple Socket.IO instances can share the message-passing load. Database connection pooling means we're not opening and closing connections constantly, which would exhaust database server capacity. We also offload some processing to clients—video encoding happens in the browser before transmission, not on our servers.

The result? Adding capacity just means deploying more identical server instances behind a load balancer. There's no complex sharding or database partitioning required until you hit truly massive scales (hundreds of thousands of concurrent users), which is well beyond our target deployment scenarios.

\subsection{Data Security}

Video storage presented both technical and regulatory challenges. A single two-hour exam session generates gigabytes of video data. Multiply that across thousands of students taking multiple exams per semester and storage costs escalate quickly. More concerning, all that data includes biometric information protected under GDPR, CCPA, and FERPA—it can't just sit on unencrypted drives indefinitely.

We implemented several layers of protection. First, configurable retention policies: administrators specify how long exam data persists (30 days, 90 days, one year), and the system automatically deletes recordings after that period expires. This satisfies data minimization requirements. Second, encryption at multiple layers: AES-256 for data at rest on disk, HTTPS/TLS for data in transit over networks, and DTLS-SRTP for WebRTC video streams. Third, comprehensive audit logging: every access to biometric data gets logged with who accessed it, when, and why. If there's ever a data breach investigation or regulatory audit, we can produce complete access histories.

\section{Feasibility Study}

\subsection{Technical Feasibility}

Before committing to full development, we needed to verify that building this system was actually feasible with available technology and reasonable hardware requirements.

The good news? Every component we needed already existed in mature, production-ready form. We weren't inventing new AI algorithms or waiting for hypothetical future hardware. The system integrates proven technologies: YOLOv8n and MediaPipe for AI, Flask for the backend API, React.js for the frontend, MySQL for data persistence, Socket.IO for real-time communication, and WebRTC for video streaming.

Hardware requirements are modest. Students need basic 720p webcams (most laptops have these built-in), a modern multi-core CPU (Intel i5, AMD Ryzen 5, or equivalent—nothing exotic), and broadband internet capable of at least 2 Mbps upload speed. Crucially, no GPU is required. YOLOv8n runs effectively on CPU-only systems, which means institutions don't need to invest in expensive machine learning infrastructure.

All the software components are open-source or freely available. Flask, React.js, MySQL, YOLOv8n, MediaPipe, Socket.IO, and WebRTC all have active development communities and permissive licenses. Security standards like HTTPS/TLS, AES-256, and JWT (RFC 7519) are well-documented and widely implemented.

\textbf{Assessment: FEASIBLE.} We found no technical blockers. Everything needed to build this platform already exists and works at scale in production environments elsewhere.

\subsection{Operational Feasibility}

Technical feasibility doesn't matter if users can't figure out how to operate the system. We needed to verify that students, teachers, and administrators could actually use this platform without extensive training.

We built the interface using React.js and Tailwind CSS, prioritizing clean, modern design over feature overload. The student view is intentionally minimal—join exam, answer questions, submit. Teachers get more complexity since they need to create exams, configure proctoring settings, monitor live sessions, and grade responses. Administrators need the full feature set for managing users, departments, and system configuration.

The platform supports multiple question types within a single exam: multiple-choice questions (automatically graded), constructed-response questions (manual grading), file uploads (for code submissions, diagrams, etc.), and mixed formats combining all three. We intentionally avoided proprietary formats—exams are just forms with validation.

Early user testing gave us confidence. We ran sessions with actual students and instructors (not developers) and tracked how long it took them to complete common tasks. Most users became proficient within 15-20 minutes. The learning curve wasn't zero, but it was shallow enough that brief training sessions would suffice.

\textbf{Assessment: FEASIBLE.} The interface is intuitive enough for real-world deployment without requiring extensive training programs or dedicated support staff.

\subsection{Economic Feasibility}

The business case had to be compelling. If this platform cost as much or more than hiring human proctors, institutions wouldn't adopt it regardless of technical merit.

We modeled costs for an institution conducting 10,000 exams annually—a medium-sized university or large community college. Table 2.2 compares traditional manual proctoring against our AI-powered platform:

\begin{table}[ht]
\centering
\caption{Cost-Benefit Analysis: Manual vs. AI-Powered Proctoring (Annual, 10,000 Exams)}
\label{tab:cost_benefit}
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Cost Category} & \textbf{Manual Proctoring} & \textbf{Our Platform} & \textbf{Savings} \\
\hline
\multicolumn{4}{|c|}{\textit{Development Costs (One-time)}} \\
\hline
Initial Development & \$0 & \$83,000 & -\$83,000 \\
\hline
\multicolumn{4}{|c|}{\textit{Annual Operating Costs}} \\
\hline
Proctor Salaries & \$250,000 & \$0 & \$250,000 \\
Infrastructure & \$5,000 & \$9,800 & -\$4,800 \\
Maintenance & \$8,000 & \$12,000 & -\$4,000 \\
Technical Support & \$12,000 & \$18,000 & -\$6,000 \\
Software Updates & \$3,000 & \$6,000 & -\$3,000 \\
\hline
\textbf{Total Annual} & \textbf{\$278,000} & \textbf{\$45,800} & \textbf{\$232,200} \\
\hline
\textbf{Cost Reduction} & \textbf{--} & \textbf{--} & \textbf{82\%} \\
\hline
\textbf{ROI Period} & \textbf{--} & \textbf{4.3 months} & \textbf{--} \\
\hline
\end{tabular}
\end{table}

The numbers tell a clear story. Manual proctoring costs \$278,000 annually, dominated by the \$250,000 in proctor salaries. Our platform requires higher infrastructure and maintenance costs but eliminates the salary expense entirely. Total annual operating cost: \$45,800—an 82\% reduction.

The upfront development cost of \$83,000 might seem high, but the ROI calculation is straightforward: \$232,200 in annual savings pays back the initial investment in just 4.3 months. After that, institutions save over \$230,000 every single year.

For smaller institutions conducting 2,000+ exams annually, the economics still work—the platform pays for itself within the first year. Below that threshold, manual proctoring might actually be cheaper depending on proctor wage rates.

\textbf{Assessment: HIGHLY FEASIBLE.} The economics strongly favor AI-powered proctoring for any institution conducting more than a few thousand exams per year.

\subsection{Legal Feasibility}

Building a system that collects biometric data raised immediate legal questions. Could we deploy this in Europe under GDPR? Would it pass California's strict CCPA requirements? Does it comply with U.S. educational privacy law (FERPA)? If the answer to any of these was "no," the platform wouldn't be viable.

We researched each regulatory framework carefully:

\textbf{GDPR (European Union)} is the strictest. It requires explicit consent before processing biometric data—we can't assume consent or use pre-checked boxes. It mandates data minimization—don't collect more than absolutely necessary. It grants users the right to access their data, request deletion, and port data to other systems. Finally, it requires "privacy by design"—security built in from the start, not added later. Our platform implements all of these: explicit consent dialogs, minimal data collection, user data access portals, automated deletion capabilities, and end-to-end encryption.

\textbf{CCPA (California, USA)} shares similar principles with slightly different requirements. We must provide notice at collection explaining what data we're gathering and why. Users have the right to know what data we've collected, request deletion, and opt out of certain processing. The law prohibits discriminating against users who exercise these rights. Our platform satisfies all requirements through transparent privacy policies, data access APIs, deletion workflows, and opt-out mechanisms that don't restrict core functionality.

\textbf{FERPA (USA Educational Records)} governs educational records specifically. It requires access controls (we use JWT with RBAC), audit trails (comprehensive logging), encryption (HTTPS/TLS and AES-256), and special consent requirements for minors under 18 years old (parental consent workflows). We implemented all of these during initial development.

Beyond legal compliance, we considered \textbf{ethical AI usage}. The platform provides algorithmic transparency—instructors can see confidence scores and understand why violations were flagged. We chose models (MediaPipe, YOLOv8n) that have been evaluated for bias and work across diverse populations. Human oversight is built in—instructors review all violations before taking action. Students can appeal bans through a formal process. Every violation includes screenshot evidence and confidence scores so decisions are explainable.

\textbf{Assessment: COMPLIANT.} The platform satisfies GDPR, CCPA, and FERPA requirements and incorporates ethical AI principles including transparency, fairness, human oversight, and explainability.

\subsection{Market Feasibility}

Technical and legal feasibility don't matter if there's no market. Who would actually use this platform? Is the market large enough to justify development effort?

The addressable market is substantial. Our platform scales from small classroom assessments (10-50 students) to large university-wide examinations (1,000+ students). Potential users include traditional universities, community colleges, corporate training programs, professional certification bodies, and online course platforms like Coursera or edX.

Market research supported strong demand. Industry analysts project the global e-learning market will reach \$375 billion by 2026 \cite{globalmarket2020}. Within that, online proctoring specifically represents a \$1.2 billion segment growing at 12\% annually \cite{marketsandmarkets2021}. Those aren't niche numbers—they indicate mainstream adoption.

Educational institutions are actively seeking better solutions. The EDUCAUSE survey we cited earlier showed 73\% of higher education institutions now offer fully online programs, with 35 million students taking remote exams annually \cite{educause2021}. More telling, 67\% of institutions report increased demand for automated proctoring solutions \cite{insidehighered2021}. They're not satisfied with current options.

Our platform offers specific advantages over competitors: real-time monitoring with sub-150ms alert latency (matching ProctorU without the cost), dual-camera architecture using hardware students already own, integrated exam management eliminating the need for separate LMS integration, zero per-exam fees for self-hosted deployments, open-source architecture allowing customization, and privacy-first design with full GDPR/CCPA compliance.

\textbf{Assessment: STRONG MARKET POTENTIAL.} A large, growing market exists with demonstrated demand for better proctoring solutions. Our platform addresses specific pain points—cost, privacy, real-time response—that existing solutions handle poorly.

\section{Research Gap and Contribution}

After reviewing the literature and analyzing commercial products, we identified several gaps between what exists and what institutions actually need:

\textbf{Dual-camera systems exist in research but not in practice.} Academic papers have proposed using multiple cameras for comprehensive monitoring \cite{hussein2020}, but practical implementations typically required expensive specialized hardware that institutions couldn't afford. Using students' existing mobile phones as the second camera—an obvious solution in retrospect—remained largely unexplored in production systems. We saw an opportunity to make dual-camera monitoring practical and affordable.

\textbf{Real-time intervention is rare.} Most academic research focuses on post-exam video analysis: record everything, then have humans or AI review it later to identify violations. That approach documents cheating but doesn't prevent it. The student has already completed the exam before anyone notices. Research exploring the trade-off between detection accuracy and response latency—how fast can you detect violations while maintaining acceptable false-positive rates?—remains limited. Our platform prioritizes real-time detection, accepting some accuracy constraints in exchange for immediate intervention capability.

\textbf{Privacy often gets sacrificed for security.} Existing commercial platforms typically collect extensive biometric data with vague retention policies and no clear deletion mechanisms. They prioritize detecting cheating (security) over protecting student privacy. Academic research on privacy-by-design proctoring architectures—systems built from the ground up to minimize data collection and maximize user control—is surprisingly sparse. We designed our platform with privacy as a first-order requirement, not a compliance checkbox.

\textbf{Economic analysis is mostly absent.} We found limited peer-reviewed research quantifying the actual costs and benefits of different proctoring approaches. Vendors publish marketing materials with optimistic projections, but rigorous academic cost-benefit analyses comparing manual proctoring, commercial SaaS platforms, and self-hosted AI solutions are rare. This made it hard to build a fact-based business case.

Our platform addresses these gaps directly. We provide an open-source, privacy-conscious, real-time proctoring solution using dual-camera monitoring with consumer hardware students already own. We conducted thorough cost-benefit analysis demonstrating 82\% cost reduction compared to manual proctoring. And we built transparent AI decision-making with human oversight and formal appeals mechanisms throughout.
